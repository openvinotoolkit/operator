# Retrieval Augmented Generation with OpenVINO Model Server demo

This demo shows how to deploy in Kubernetes or OpenShift a service based on OpenVINO Model Server with a generative use cases. OpenVINO Model Server has a feature of serving MediaPipe graphs. Here we are going to use a graph with a Python calculator analyzing documents based on RAG algorithm. It indexes the documents provided to the service and expose KServe gRPC endpoint which can be used to query about content. The response is generated by the server using LLM model and Hugging Face library powered by OpenVINO Runtime.

Here are the steps we are going to follow:
- building the container image of the model server together with required python dependencies (public image has minimal server of python components)
- adding the RAG servable configuration to a ConfigMap (it includes the model server configuraiton, MediaPipe graph, python code and also a list of documents to be analysed)
- optionally adding Hugging Face token to a secret, it will be attached to the deployed service to pull the models in case they requires authentication
- deploying the service via the operator and ModelServer custom resource
- running the client to query the document
- online updates of the documents and reruning the client query


## Building the container image of the model server

The public image of Model Server is enabled for python execution but it has a minimal list of python dependencies. Depending on the use cases, a custom dependencies might need to be included in the image. Follow the instructions from https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos/rag_chatbot

```
git clone https://github.com/openvinotoolkit/model_server.git
cd model_server
# for ubi base image
make python_image BASE_OS=redhat OVMS_CPP_DOCKER_IMAGE=registry.connect.redhat.com/intel/openvino-model-server OVMS_CPP_IMAGE_TAG=2024.1-gpu 
# or for ubuntu22 base image
make python_image BASE_OS=redhat OVMS_CPP_DOCKER_IMAGE=openvino/model_server OVMS_CPP_IMAGE_TAG=2024.1-gpu
```
Push the build image to your image registry.

In case of OpenShift, this build operation can be automated using a BuildConfig resource.

```
spec:
  nodeSelector: null
  output:
    to:
      kind: ImageStreamTag
      name: 'ovms:py'
  resources: {}
  strategy:
    type: Docker
    dockerStrategy:
      dockerfilePath: Dockerfile.redhat
      buildArgs:
        - name: IMAGE_NAME
          value: 'registry.connect.redhat.com/intel/openvino-model-server@sha256:b8721a65da98bd9b354680ecb2dca8a4d60a1097babe9822ab264f8e951190f7'
  postCommit: {}
  source:
    type: Git
    git:
      uri: 'https://github.com/openvinotoolkit/model_server'
      ref: main
    contextDir: demos/python_demos
```
It will trigger the build based on provided above Dockerfile and the based image. That image will be automatically pushed to the local cluster container registry in the OpenShift.

## Adding the RAG servable configuration to a configmap

The Model Server configuration including servable configuration needs to be attached to deployed pods. In this demo they will be passed via a configmap which can stored arbitrary files. The operator automatically mounts the contnent of a configmap to the serving pod.

```
pushd $(pwd)
cd model_server/demos/python_demos/rag_chatbot/servable_stream
echo "https://gist.githubusercontent.com/ryanloney/42b8ebe29f95ebd4382ee0b2bb50bea2/raw/cfbb679fefb6babec675c7806254a5fff29a5e6b/aipc.txt" > docs.txt
oc create configmap rag-demo --from-file=config.json=config.json --from-file=graph.pbtxt=graph.pbtxt --from-file=model.py=model.py \
--from-file=config.py=config.py --from-file=ov_embedding_model.py=ov_embedding_model.py \
--from-file=ov_llm_model.py=ov_llm_model.py --from-file=docs.txt=docs.txt
popd
```

## Adding Hugging Face token to a Secret and the model name to the ConfigMap
The RAG demo can work with several models and it can be defined as an environment variable passed to the pod. Some models might require a token to authorize the model pull from Hugging Face hub. Extra environment variables can be added to the deployment via a ConfigMap or a secret.

```bash
oc create secret generic rag-env --from-literal=HF_TOKEN=hf_GFb...
oc create configmap rag-env --from-literal=SELECTED_MODEL=tiny-llama-2-chat-7b
```

## Deploying the service

Now we can deploy the Model Server by applying the custom resource:
```bash
kubectl apply -f - <<EOF
apiVersion: intel.com/v1alpha1
kind: ModelServer
metadata:
  name: ovms-rag
spec:
  image_name: registry.toolbox.iotg.sclab.intel.com/cpp/model_server-gpu:py
  deployment_parameters:
    replicas: 1
    extra_envs_secret: "rag-env"
    extra_envs_configmap: "rag-env"
  models_settings:
    single_model_mode: false
    config_configmap_name: "rag-demo"
  server_settings:
    log_level: "INFO"
  service_parameters:
    grpc_port: 8080
    rest_port: 8081
EOF
```
It will deploy new pod and service `ovms-rag`. In the pod initialization, it will pull and load the LLM and embedding models from Hugging Face. Then the documents from provided docs.txt file will be downloaded and analyzed. The server will be updating the analysis when the docs.txt file is modified.

## Running the client

Confirm if the RAG servable is ready for processing. Below is a command to be executed from a client Pod in the same namespace:
```bash
curl http://ovms-rag:8081/v2/models/python_model/ready
```

The query can be also executed via a gRPC client with steaming capabilities. That way you can read the text as it gets generated.

```bash
pushd $(pwd)
cd model_server/demos/python_demos/llm_text_generation
pip install -r client_requirements.txt
python3 client_stream.py --url ovms-rag:8080 --prompt "Summarize what is AIPC."
popd
```

## Updating the documents
Sometimes you might want to change the scope of the RAG analysis and change the documents. It can be done without reloading of the model and restarting the service.
Just the `docs.txt` needs to be updated. The RAG servable has an extra thread checking regularly if the file is modified. In such case documentes are downloaded and indexed again. 

```bash
pushd $(pwd)
cd model_server/demos/python_demos/rag_chatbot/servable_stream
echo "https://gist.githubusercontent.com/dtrawins/2956a7a77aa6732b52b8ae6eab0be205/raw/e05f2ab8fea9c8631ac5f20b8dd640074ae429c7/genai.txt" > docs.txt
oc delete configmap rag-demo && \
oc create configmap rag-demo --from-file=config.json=config.json --from-file=graph.pbtxt=graph.pbtxt --from-file=model.py=model.py \
--from-file=config.py=config.py --from-file=ov_embedding_model.py=ov_embedding_model.py \
--from-file=ov_llm_model.py=ov_llm_model.py --from-file=docs.txt=docs.txt
popd
```

Wait a few moments and rerun the query:
```bash
pushd $(pwd)
cd model_server/demos/python_demos/llm_text_generation
python3 client_stream.py --url ovms-rag:8080 --prompt "What are the features of Gaudi3?"
popd
```


## Deploying on GPU

The same demo can be adjusted to run the inference on GPU cards. It would require the following changes:
- installing Intel Device Plugin
- adding to the ModelServer resource requirements
- adding extra environment variable DEVICE=gpu

## Deploying on Persistent Volume Claim

The demo can be used also with the cluster storage and PVC in the cluster. In such scenario the model can be stored in the same location with the servable configuration files. That way model downloading Hugging Face hub is needed. Locally stored model can be in compressed IR format. It speeds up the model loading time.

