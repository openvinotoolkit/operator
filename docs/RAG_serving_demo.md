# Retrieval Augmented Generation with OpenVINO Model Server demo

This demo shows how to deploy in Kubernetes or OpenShift a service based on OpenVINO Model Server with a generative use cases. OpenVINO Model Server has a feature of serving MediaPipe graphs. Here we are going to use a graph with a Python calculator analyzing documents based on RAG algorithm. It indexes the documents provided to the service and expose KServe gRPC endpoint which can be used to query about content. The response is generated by the server using LLM model and Hugging Face library powered by OpenVINO Runtime.

Here are the steps we are going to follow:
- building the container image of the model server together with required python dependencies (public image has minimal server of python components)
- adding the RAG servable configuration to a ConfigMap (it includes the model server configuraiton, MediaPipe graph, python code and also a list of documents to be analysed)
- optionally adding Hugging Face token to a secret, it will be attached to the deployed service to pull the models in case they requires authentication
- deploying the service via the operator and ModelServer custom resource
- running the client to query the document
- online updates of the documents and rerunning the client query


## Building the container image of the model server

The public image of Model Server is enabled for python execution but it has a minimal list of python dependencies. Depending on the use cases, a custom dependencies might need to be included in the image. Follow the instructions from https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos/rag_chatbot

```bash
git clone https://github.com/openvinotoolkit/model_server.git
cd model_server
# for ubi base image
make python_image BASE_OS=redhat OVMS_CPP_DOCKER_IMAGE=registry.connect.redhat.com/intel/openvino-model-server OVMS_CPP_IMAGE_TAG=2024.1-gpu 
# or for ubuntu22 base image
make python_image BASE_OS=ubuntu OVMS_CPP_DOCKER_IMAGE=openvino/model_server OVMS_CPP_IMAGE_TAG=2024.1-gpu
```
Push the build image to your image registry.

In case of OpenShift, this build operation can be automated using a BuildConfig resource. After creating an ImageStream called `ovms`, create build config with the below spec:

```
spec:
  nodeSelector: null
  output:
    to:
      kind: ImageStreamTag
      name: 'ovms:py'
  resources: {}
  strategy:
    type: Docker
    dockerStrategy:
      dockerfilePath: Dockerfile.redhat
      buildArgs:
        - name: IMAGE_NAME
          value: 'registry.connect.redhat.com/intel/openvino-model-server@sha256:b8721a65da98bd9b354680ecb2dca8a4d60a1097babe9822ab264f8e951190f7'
  postCommit: {}
  source:
    type: Git
    git:
      uri: 'https://github.com/openvinotoolkit/model_server'
      ref: main
    contextDir: demos/python_demos
```
After triggering the build for that configuration, an image will be automatically pushed to the local cluster container registry in the OpenShift.

## Adding the RAG servable configuration to a ConfigMap

The Model Server configuration including servable configuration needs to be attached to deployed pods. In this demo they will be passed via a ConfigMap which can store arbitrary files. The operator automatically mounts the content of a ConfigMap to the serving pod.

```bash
pushd $(pwd)
cd model_server/demos/python_demos/rag_chatbot/servable_stream
echo https://gist.githubusercontent.com/ryanloney/42b8ebe29f95ebd4382ee0b2bb50bea2/raw/cfbb679fefb6babec675c7806254a5fff29a5e6b/aipc.txt > docs.txt
oc create configmap rag-demo --from-file=config.json=config.json --from-file=graph.pbtxt=graph.pbtxt --from-file=model.py=model.py \
--from-file=config.py=config.py --from-file=ov_embedding_model.py=ov_embedding_model.py \
--from-file=ov_llm_model.py=ov_llm_model.py --from-file=docs.txt=docs.txt
popd
```

## Adding Hugging Face token to a Secret and the model name to the ConfigMap
The RAG demo can work with several models and it can be defined as an environment variable passed to the pod. Some models might require a token to authorize the model pull from Hugging Face hub. Extra environment variables can be added to the deployment via a ConfigMap or a secret.

```bash
oc create secret generic rag-env --from-literal=HF_TOKEN=hf_GFb...
oc create configmap rag-env --from-literal=SELECTED_MODEL=tiny-llama-2-chat-7b
```

## Deploying the service

Now we can deploy the Model Server by applying the custom resource:
```bash
oc apply -f - <<EOF
apiVersion: intel.com/v1alpha1
kind: ModelServer
metadata:
  name: ovms-rag
spec:
  image_name: image-registry.openshift-image-registry.svc:5000/<your project name>/ovms:py
  deployment_parameters:
    replicas: 1
    extra_envs_secret: "rag-env"
    extra_envs_configmap: "rag-env"
  models_settings:
    single_model_mode: false
    config_configmap_name: "rag-demo"
  server_settings:
    log_level: "INFO"
  service_parameters:
    grpc_port: 8080
    rest_port: 8081
EOF
```
It will deploy new pod and service `ovms-rag`. In the pod initialization, it will pull and load the LLM and embedding models from Hugging Face. Then the documents from provided docs.txt file will be downloaded and analyzed. The server will be updating the analysis when the docs.txt file is modified.

## Running the client

Confirm if the RAG servable is ready for processing. Below is a command to be executed from a client Pod in the same namespace:
```bash
curl http://ovms-rag:8081/v2/models/python_model/ready
```

The query can be also executed via a gRPC client with steaming capabilities. That way you can read the text as it gets generated.

```bash
pushd $(pwd)
cd model_server/demos/python_demos/llm_text_generation
pip install -r client_requirements.txt
python3 client_stream.py --url ovms-rag:8080 --prompt "Summarize what is AIPC."
popd
```

Alternatively, deploy a service based on `gradio` component, which provide easy to use GUI interface.
In Openshift you can build the image with gradio client using the following BuildConfig spec. It assumes the image stream name `rag-gradio`:
```
spec:
  nodeSelector: null
  output:
    to:
      kind: ImageStreamTag
      name: 'rag-gradio:latest'
  resources: {}
  successfulBuildsHistoryLimit: 5
  failedBuildsHistoryLimit: 5
  strategy:
    type: Docker
    dockerStrategy:
      dockerfilePath: Dockerfile.gradio
  postCommit: {}
  source:
    type: Git
    git:
      uri: 'https://github.com/openvinotoolkit/model_server'
      ref: main
    contextDir: demos/python_demos/rag_chatbot
```

Launch the pod:
```bash
oc run gradio --image image-registry.openshift-image-registry.svc:5000/<your project name>/rag-gradio --port 9000 --command -- python app.py --ovms_url ovms-rag:8080 --web_url 0.0.0.0:7860

oc apply -f - <<EOF
kind: Service
apiVersion: v1
metadata:
  name: rag-gradio
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 7860
  type: ClusterIP
  selector:
    run: gradio
EOF
```
The created service can be not exposed via Routes resources to connect to gradio interface from the browser:

![gradio](./rag-gradio.png)

## Updating the documents
Sometimes you might want to change the scope of the RAG analysis and change the documents. It can be done without reloading of the model and restarting the service.
Just the `docs.txt` needs to be updated. The RAG servable has an extra thread checking regularly if the file is modified. In such case documentes are downloaded and indexed again. 

```bash
pushd $(pwd)
cd model_server/demos/python_demos/rag_chatbot/servable_stream
echo https://gist.githubusercontent.com/dtrawins/2956a7a77aa6732b52b8ae6eab0be205/raw/e05f2ab8fea9c8631ac5f20b8dd640074ae429c7/genai.txt > docs.txt
oc delete configmap rag-demo && \
oc create configmap rag-demo --from-file=config.json=config.json --from-file=graph.pbtxt=graph.pbtxt --from-file=model.py=model.py \
--from-file=config.py=config.py --from-file=ov_embedding_model.py=ov_embedding_model.py \
--from-file=ov_llm_model.py=ov_llm_model.py --from-file=docs.txt=docs.txt
popd
```

Wait a few moments and rerun the query:
```bash
pushd $(pwd)
cd model_server/demos/python_demos/llm_text_generation
python3 client_stream.py --url ovms-rag:8080 --prompt "What are the features of Gaudi3?"
popd
```

## Deploying on GPU

The same demo can be adjusted to run the inference on GPU cards. It would require the following changes:
- installing [Intel Device Plugin for Kubernetes](https://github.com/intel/intel-device-plugins-for-kubernetes)
- adding to the ModelServer resource requirements
- adding extra environment variable DEVICE=gpu
  
```
deployment_parameters:
  resources:
    limits:
      xpu_device: gpu.intel.com/i915
      xpu_device_quantity: "1"
```
- added extra environment variable `DEVICE=gpu` to the configmap `rag-env`.

## Deploying on Persistent Volume Claim

The demo can be used also with the cluster storage and PVC in the cluster. In such scenario the model can be stored in the same location with the servable configuration files. That way model downloading Hugging Face hub is needed. Locally stored model can be in compressed IR format. It speeds up the model loading time.

In such scenario, copy the content of `servable_stream` from the [RAG demo](https://github.com/openvinotoolkit/model_server/tree/main/demos/python_demos/rag_chatbot) to the volume claim togather with the downloaded and compressed models in IR format.
It can be then mounted to the model server containers using the parameter with dropped `config_configmap_name`:

```
models_repository:
  models_volume_claim: <pvc name>
models_settings:
  config_path: /models/<path in pvc>/config.json
```



